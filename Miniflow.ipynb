{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) WELCOME TO MINIFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "\n",
    "\n",
    "1) Differentiable Graphs\n",
    "\n",
    "2) Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) GRAPHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodes and edges create a graph structure\n",
    "\n",
    "There are generally two steps to create neural networks:\n",
    "\n",
    "1)Define the graph of nodes and edges.\n",
    "\n",
    "2)Propagate values through the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) MINIFLOW ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node will need to be able to pass values forward and perform backpropagation (more on that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Node(s) from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Node(s) to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # For each inbound Node here, add this Node as an outbound Node to _that_ Node.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Node defines the base set of properties that every node holds, only specialized subclasses of Node will end up in the graph. As part of this lab, you'll build the subclasses of Node that can perform calculations and hold values. For example, consider the Input subclass of Node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Input(Node): #Input(Node) \n",
    "    def __init__(self):\n",
    "        # An Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    # NOTE: Input node is the only node where the value\n",
    "    # may be passed as an argument to forward().\n",
    "    #\n",
    "    # All other node implementations should get the value\n",
    "    # of the previous node from self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            \n",
    "class Add(Node): #Add(Node) \n",
    "    def __init__(self, x, y):\n",
    "        Node.__init__(self, [x, y])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        You'll be writing code here in the next quiz!\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4) FORWARD PROPAGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MiniFlow has two methods to help you define and then run values through your graphs: \n",
    "\n",
    "topological_sort() ->  returns a sorted list of nodes in which all of the calculations can run in series\n",
    "\n",
    "forward_pass()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Quiz 1 - Passing Values Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to define your network, you'll need to define the order of operations for your nodes: Topological sort\n",
    "\n",
    "The other method at your disposal is forward_pass(), which actually runs the network and outputs a value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1) Open nn.py below. You don't need to change anything. I just want you to see how MiniFlow works.\n",
    "\n",
    "2) Open miniflow.py. Finish the forward method on the Add class. All that's required to pass this quiz is a correct implementation of forward.\n",
    "\n",
    "3) Test your network by hitting \"Test Run!\" When the output looks right, hit \"Submit!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You need to change the Add() class below.\n",
    "\"\"\"\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Nodes from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Nodes to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # an Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    # NOTE: Input node is the only node that may\n",
    "    # receive its value as an argument to forward().\n",
    "    #\n",
    "    # All other node implementations should calculate their\n",
    "    # values from the value of previous nodes, using\n",
    "    # self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, x, y):\n",
    "        # You could access `x` and `y` in forward with\n",
    "        # self.inbound_nodes[0] (`x`) and self.inbound_nodes[1] (`y`)\n",
    "        Node.__init__(self, [x, y])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node (`self.value`) to the sum of its inbound_nodes.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        x_value = self.inbound_nodes[0].value\n",
    "        y_value = self.inbound_nodes[1].value\n",
    "        self.value = x_value + y_value\n",
    "\n",
    "\"\"\"\n",
    "No need to change anything below here!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 + 5 = 15 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "x, y = Input(), Input()\n",
    "\n",
    "f = Add(x, y)\n",
    "\n",
    "feed_dict = {x: 10, y: 5}\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "output = forward_pass(f, sorted_nodes)\n",
    "\n",
    "# print (f.inbound_nodes[0].value)\n",
    "\n",
    "# NOTE: because topological_sort set the values for the `Input` nodes we could also access\n",
    "# the value for x with x.value (same goes for y).\n",
    "print(\"{} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Bonus Challenges!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1) Can you make Add accept any number of inputs? Eg. Add(x, y, z).\n",
    "\n",
    "2) Can you make a Mul class that multiplies n inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Nodes from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Nodes to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # An Input Node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    # NOTE: Input Node is the only Node where the value\n",
    "    # may be passed as an argument to forward().\n",
    "    #\n",
    "    # All other Node implementations should get the value\n",
    "    # of the previous nodes from self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    # You may need to change this...\n",
    "    def __init__(self, *inputs):\n",
    "        Node.__init__(self, inputs)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        For reference, here's the old way from the last\n",
    "        quiz. You'll want to write code here.\n",
    "        \"\"\"\n",
    "        \n",
    "        #x_value = self.inbound_nodes[0].value\n",
    "        #y_value = self.inbound_nodes[1].value\n",
    "        #self.value = x_value + y_value\n",
    "        \n",
    "        self.value = 0\n",
    "        for item in self.inbound_nodes:\n",
    "            self.value += item.value\n",
    "\n",
    "class Mul(Node):\n",
    "    def __init__(self, *inputs):\n",
    "        Node.__init__(self, inputs)\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        self.value = 0\n",
    "        for item in self.inbound_nodes:\n",
    "            self.value *= item.value\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 + 5 + 10 = 19 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "No need to change anything here!\n",
    "\n",
    "If all goes well, this should work after you\n",
    "modify the Add class in miniflow.py.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "x, y, z = Input(), Input(), Input()\n",
    "\n",
    "f = Add(x, y, z)\n",
    "\n",
    "feed_dict = {x: 4, y: 5, z: 10}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "# should output 19\n",
    "print(\"{} + {} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], feed_dict[z], output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6) THE LINEAR FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To explore why accuracy matters, I want you to first implement a trickier (and more useful!) node than Add: the Linear node.\n",
    "\n",
    "In this next quiz, you'll try to build a linear neuron that generates an output by applying a simplified version of the weighted sum. Linear should take an list of inbound nodes of length n, a list of weights of length n, and a bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1) Open nn.py below. Read through the neural network to see the expected output of Linear.\n",
    "\n",
    "2) Open miniflow.py below. Modify Linear, which is a subclass of Node, to generate an output with y=∑w\n",
    "​i\n",
    "​​ x\n",
    "​i\n",
    "​​ +b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write the Linear#forward method below!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Nodes from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Nodes to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # An Input Node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "        # NOTE: Input Node is the only Node where the value\n",
    "        # may be passed as an argument to forward().\n",
    "        #\n",
    "        # All other Node implementations should get the value\n",
    "        # of the previous nodes from self.inbound_nodes\n",
    "        #\n",
    "        # Example:\n",
    "        # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Node.__init__(self, [inputs, weights, bias])\n",
    "\n",
    "        # NOTE: The weights and bias properties here are not\n",
    "        # numbers, but rather references to other nodes.\n",
    "        # The weight and bias values are stored within the\n",
    "        # respective nodes.\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set self.value to the value of the linear function output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        inputs = self.inbound_nodes[0].value\n",
    "        weights = self.inbound_nodes[1].value\n",
    "        bias = self.inbound_nodes[2]\n",
    "        \n",
    "        self.value = bias.value\n",
    "        for x, w in zip(inputs, weights):\n",
    "            self.value += x * w\n",
    "\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.7\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE: Here we're using an Input node for more than a scalar.\n",
    "In the case of weights and inputs the value of the Input node is\n",
    "actually a python list!\n",
    "\n",
    "In general, there's no restriction on the values that can be passed to an Input node.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "feed_dict = {\n",
    "    inputs: [6, 14, 3],\n",
    "    weights: [0.5, 0.25, 1.4],\n",
    "    bias: 2\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output) # should be 12.7 with this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 7) LINEAR TRANSFORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "I want you to rebuild Linear to handle matrices and vectors using the venerable Python math package numpy to make your life easier. numpy is often abbreviated as np, so we'll refer to it as np when referring to code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1) Open nn.py. See how the neural network implements the Linear node.\n",
    "\n",
    "2) Open miniflow.py. Implement Equation (2) within the forward pass for the Linear node.\n",
    "\n",
    "3) Test your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modify Linear#forward so that it linearly transforms\n",
    "input matrices, weights matrices and a bias vector to\n",
    "an output.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        self.value = None\n",
    "        self.outbound_nodes = []\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward():\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    While it may be strange to consider an input a node when\n",
    "    an input is only an individual node in a node, for the sake\n",
    "    of simpler code we'll still use Node as the base class.\n",
    "\n",
    "    Think of Input as collating many individual input nodes into\n",
    "    a Node.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # An Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        # Notice the ordering of the input nodes passed to the\n",
    "        # Node constructor.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the linear transform output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        inputs = self.inbound_nodes[0].value\n",
    "        weights = self.inbound_nodes[1].value\n",
    "        bias = self.inbound_nodes[2].value\n",
    "        \n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A Node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: a topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.  4.]\n",
      " [-9.  4.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The setup is similar to the prevous `Linear` node you wrote\n",
    "except you're now using NumPy arrays instead of python lists.\n",
    "\n",
    "Update the Linear class in miniflow.py to work with\n",
    "numpy vectors (arrays) and matrices.\n",
    "\n",
    "Test your code here!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[-9., 4.],\n",
    "[-9., 4.]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 8) SIGMOID FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1) Open nn.py to see how the network will use Sigmoid.\n",
    "\n",
    "2) Open miniflow.py. Modify the forward method of the Sigmoid class to reflect the sigmoid function's behavior.\n",
    "\n",
    "3) Test your work! Hit \"Submit\" when your Sigmoid works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fix the Sigmoid class so that it computes the sigmoid function\n",
    "on the forward pass!\n",
    "\n",
    "Scroll down to get started.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        self.value = None\n",
    "        self.outbound_nodes = []\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward():\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # An Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        # Notice the ordering of the input nodes passed to the\n",
    "        # Node constructor.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    You need to fix the `_sigmoid` and `forward` methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used later with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "\n",
    "        Return the result of the sigmoid function.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x)) # the `.` ensures that `1` is a float\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the result of the\n",
    "        sigmoid function, `_sigmoid`.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "        \n",
    "        # This is a dummy value to prevent numpy errors\n",
    "        # if you test without changing this method.\n",
    "        # self.value = -1\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A Node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: a topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.23394576e-04   9.82013790e-01]\n",
      " [  1.23394576e-04   9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This network feeds the output of a linear transform\n",
    "to the sigmoid function.\n",
    "\n",
    "Finish implementing the Sigmoid class in miniflow.py!\n",
    "\n",
    "Feel free to play around with this network, too!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 10) COST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many techniques for defining the accuracy of a neural network, all of which center on the network's ability to produce values that come as close as possible to known correct values. People use different names for this accuracy measurement, often terming it loss or cost. I'll use the term cost most often.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](codecogseqn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For this lab, you will calculate the cost using the mean squared error (MSE). It looks like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1) Check out nn.py to see how MSE will calculate the cost.\n",
    "\n",
    "2) Open miniflow.py. Finish building MSE.\n",
    "\n",
    "3) Test your network! See if the cost makes sense given the inputs by playing with nn.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        # TODO: your code here\n",
    "        m = self.inbound_nodes[0].value.shape[0]\n",
    "\n",
    "        diff = y - a\n",
    "        self.value = np.mean(diff**2)\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.4166666667\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test your MSE method with this script!\n",
    "\n",
    "No changes necessary, but feel free to play\n",
    "with this script to test your network.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a)\n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass(graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 11) GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For this quiz you'll complete TODOs in both the f.py and gd.py files.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "Set the learning_rate in f.py.\n",
    "Complete the gradient descent implementation in gradient_descent_update function in gd.py.\n",
    "Notes:\n",
    "\n",
    "Setting the learning_rate to 0.1 should result in x -> 0 and f(x) -> 5 if you've implemented gradient descent correctly.\n",
    "Play around with different values for the learning rate. Try very small values, values close to 1, above 1, etc. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_update(x, gradx, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient descent update.\n",
    "    \"\"\"\n",
    "    # TODO: Implement gradient descent.\n",
    "    x = x - learning_rate * gradx\n",
    "    # Return the new value for x\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0: Cost = 93876726.000, x = 19378.000\n",
      "EPOCH 1: Cost = 93501594.623, x = 19339.244\n",
      "EPOCH 2: Cost = 93127962.271, x = 19300.566\n",
      "EPOCH 3: Cost = 92755822.953, x = 19261.964\n",
      "EPOCH 4: Cost = 92385170.705, x = 19223.440\n",
      "EPOCH 5: Cost = 92015999.583, x = 19184.994\n",
      "EPOCH 6: Cost = 91648303.668, x = 19146.624\n",
      "EPOCH 7: Cost = 91282077.067, x = 19108.330\n",
      "EPOCH 8: Cost = 90917313.907, x = 19070.114\n",
      "EPOCH 9: Cost = 90554008.341, x = 19031.973\n",
      "EPOCH 10: Cost = 90192154.543, x = 18993.910\n",
      "EPOCH 11: Cost = 89831746.714, x = 18955.922\n",
      "EPOCH 12: Cost = 89472779.074, x = 18918.010\n",
      "EPOCH 13: Cost = 89115245.869, x = 18880.174\n",
      "EPOCH 14: Cost = 88759141.366, x = 18842.413\n",
      "EPOCH 15: Cost = 88404459.857, x = 18804.729\n",
      "EPOCH 16: Cost = 88051195.656, x = 18767.119\n",
      "EPOCH 17: Cost = 87699343.098, x = 18729.585\n",
      "EPOCH 18: Cost = 87348896.543, x = 18692.126\n",
      "EPOCH 19: Cost = 86999850.372, x = 18654.742\n",
      "EPOCH 20: Cost = 86652198.990, x = 18617.432\n",
      "EPOCH 21: Cost = 86305936.823, x = 18580.197\n",
      "EPOCH 22: Cost = 85961058.319, x = 18543.037\n",
      "EPOCH 23: Cost = 85617557.950, x = 18505.951\n",
      "EPOCH 24: Cost = 85275430.209, x = 18468.939\n",
      "EPOCH 25: Cost = 84934669.609, x = 18432.001\n",
      "EPOCH 26: Cost = 84595270.690, x = 18395.137\n",
      "EPOCH 27: Cost = 84257228.008, x = 18358.347\n",
      "EPOCH 28: Cost = 83920536.145, x = 18321.630\n",
      "EPOCH 29: Cost = 83585189.702, x = 18284.987\n",
      "EPOCH 30: Cost = 83251183.304, x = 18248.417\n",
      "EPOCH 31: Cost = 82918511.596, x = 18211.920\n",
      "EPOCH 32: Cost = 82587169.243, x = 18175.496\n",
      "EPOCH 33: Cost = 82257150.935, x = 18139.145\n",
      "EPOCH 34: Cost = 81928451.380, x = 18102.867\n",
      "EPOCH 35: Cost = 81601065.308, x = 18066.661\n",
      "EPOCH 36: Cost = 81274987.471, x = 18030.528\n",
      "EPOCH 37: Cost = 80950212.641, x = 17994.467\n",
      "EPOCH 38: Cost = 80626735.612, x = 17958.478\n",
      "EPOCH 39: Cost = 80304551.196, x = 17922.561\n",
      "EPOCH 40: Cost = 79983654.229, x = 17886.716\n",
      "EPOCH 41: Cost = 79664039.567, x = 17850.942\n",
      "EPOCH 42: Cost = 79345702.085, x = 17815.240\n",
      "EPOCH 43: Cost = 79028636.679, x = 17779.610\n",
      "EPOCH 44: Cost = 78712838.267, x = 17744.051\n",
      "EPOCH 45: Cost = 78398301.785, x = 17708.563\n",
      "EPOCH 46: Cost = 78085022.192, x = 17673.145\n",
      "EPOCH 47: Cost = 77772994.463, x = 17637.799\n",
      "EPOCH 48: Cost = 77462213.597, x = 17602.524\n",
      "EPOCH 49: Cost = 77152674.611, x = 17567.318\n",
      "EPOCH 50: Cost = 76844372.544, x = 17532.184\n",
      "EPOCH 51: Cost = 76537302.451, x = 17497.119\n",
      "EPOCH 52: Cost = 76231459.410, x = 17462.125\n",
      "EPOCH 53: Cost = 75926838.518, x = 17427.201\n",
      "EPOCH 54: Cost = 75623434.892, x = 17392.347\n",
      "EPOCH 55: Cost = 75321243.666, x = 17357.562\n",
      "EPOCH 56: Cost = 75020259.996, x = 17322.847\n",
      "EPOCH 57: Cost = 74720479.057, x = 17288.201\n",
      "EPOCH 58: Cost = 74421896.043, x = 17253.625\n",
      "EPOCH 59: Cost = 74124506.166, x = 17219.117\n",
      "EPOCH 60: Cost = 73828304.660, x = 17184.679\n",
      "EPOCH 61: Cost = 73533286.774, x = 17150.310\n",
      "EPOCH 62: Cost = 73239447.780, x = 17116.009\n",
      "EPOCH 63: Cost = 72946782.967, x = 17081.777\n",
      "EPOCH 64: Cost = 72655287.642, x = 17047.614\n",
      "EPOCH 65: Cost = 72364957.133, x = 17013.518\n",
      "EPOCH 66: Cost = 72075786.784, x = 16979.491\n",
      "EPOCH 67: Cost = 71787771.960, x = 16945.532\n",
      "EPOCH 68: Cost = 71500908.043, x = 16911.641\n",
      "EPOCH 69: Cost = 71215190.435, x = 16877.818\n",
      "EPOCH 70: Cost = 70930614.554, x = 16844.062\n",
      "EPOCH 71: Cost = 70647175.838, x = 16810.374\n",
      "EPOCH 72: Cost = 70364869.743, x = 16776.754\n",
      "EPOCH 73: Cost = 70083691.744, x = 16743.200\n",
      "EPOCH 74: Cost = 69803637.331, x = 16709.714\n",
      "EPOCH 75: Cost = 69524702.017, x = 16676.294\n",
      "EPOCH 76: Cost = 69246881.327, x = 16642.942\n",
      "EPOCH 77: Cost = 68970170.810, x = 16609.656\n",
      "EPOCH 78: Cost = 68694566.027, x = 16576.436\n",
      "EPOCH 79: Cost = 68420062.561, x = 16543.284\n",
      "EPOCH 80: Cost = 68146656.011, x = 16510.197\n",
      "EPOCH 81: Cost = 67874341.994, x = 16477.177\n",
      "EPOCH 82: Cost = 67603116.143, x = 16444.222\n",
      "EPOCH 83: Cost = 67332974.111, x = 16411.334\n",
      "EPOCH 84: Cost = 67063911.566, x = 16378.511\n",
      "EPOCH 85: Cost = 66795924.196, x = 16345.754\n",
      "EPOCH 86: Cost = 66529007.703, x = 16313.063\n",
      "EPOCH 87: Cost = 66263157.808, x = 16280.436\n",
      "EPOCH 88: Cost = 65998370.249, x = 16247.876\n",
      "EPOCH 89: Cost = 65734640.782, x = 16215.380\n",
      "EPOCH 90: Cost = 65471965.177, x = 16182.949\n",
      "EPOCH 91: Cost = 65210339.224, x = 16150.583\n",
      "EPOCH 92: Cost = 64949758.729, x = 16118.282\n",
      "EPOCH 93: Cost = 64690219.513, x = 16086.045\n",
      "EPOCH 94: Cost = 64431717.416, x = 16053.873\n",
      "EPOCH 95: Cost = 64174248.293, x = 16021.766\n",
      "EPOCH 96: Cost = 63917808.016, x = 15989.722\n",
      "EPOCH 97: Cost = 63662392.476, x = 15957.743\n",
      "EPOCH 98: Cost = 63407997.575, x = 15925.827\n",
      "EPOCH 99: Cost = 63154619.237, x = 15893.975\n",
      "EPOCH 100: Cost = 62902253.398, x = 15862.188\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given the starting point of any `x` gradient descent\n",
    "should be able to find the minimum value of x for the\n",
    "cost function `f` defined below.\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Quadratic function.\n",
    "\n",
    "    It's easy to see the minimum value of the function\n",
    "    is 5 when is x=0.\n",
    "    \"\"\"\n",
    "    return x**2 + 5\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"\n",
    "    Derivative of `f` with respect to `x`.\n",
    "    \"\"\"\n",
    "    return 2*x\n",
    "\n",
    "\n",
    "# Random number better 0 and 10,000. Feel free to set x whatever you like.\n",
    "x = random.randint(0, 10000)\n",
    "# TODO: Set the learning rate\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    cost = f(x)\n",
    "    gradx = df(x)\n",
    "    print(\"EPOCH {}: Cost = {:.3f}, x = {:.3f}\".format(i, cost, gradx))\n",
    "    x = gradient_descent_update(x, gradx, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 12) BACKPROPAGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Setup\n",
    "Here's the derivative of the sigmoid function w.r.t x:\n",
    "\n",
    "sigmoid(x)=1/(1+exp(−x))\n",
    "\n",
    "​∂x\n",
    "​\n",
    "​∂sigmoid\n",
    "​​ =sigmoid(x)∗(1−sigmoid(x))\n",
    "\n",
    "Complete the implementation of backpropagation for the Sigmoid node by finishing the backward method in miniflow.py.\n",
    "The backward methods for all other nodes have already been implemented. Taking a look at them might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement the backward method of the Sigmoid node.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] += grad_cost * 1\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            \"\"\"\n",
    "            TODO: Your code goes here!\n",
    "\n",
    "            Set the gradients property to the gradients with respect to each input.\n",
    "\n",
    "            NOTE: See the Linear node and MSE node for examples.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Initialize the gradients to 0.\n",
    "            self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "            # Sum the derivative with respect to the input over all the outputs.\n",
    "            for n in self.outbound_nodes:\n",
    "                grad_cost = n.gradients[self]\n",
    "                sigmoid = self.value\n",
    "                self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "\n",
    "        This is the final node of the network so outbound nodes\n",
    "        are not a concern.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
      "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
      "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
      "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test your network here!\n",
    "\n",
    "No need to change this code, but feel free to tweak it\n",
    "to test your network!\n",
    "\n",
    "Make your changes to backward method of the Sigmoid class in miniflow.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "y = Input()\n",
    "f = Linear(X, W, b)\n",
    "a = Sigmoid(f)\n",
    "cost = MSE(y, a)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2.], [3.]])\n",
    "b_ = np.array([-3.])\n",
    "y_ = np.array([1, 2])\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W: W_,\n",
    "    b: b_,\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_and_backward(graph)\n",
    "# return the gradients for each Input\n",
    "gradients = [t.gradients[t] for t in [X, y, W, b]]\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
    "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
    "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
    "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n",
    "\"\"\"\n",
    "print(gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 13) STOCHASTIC GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) is a version of Gradient Descent where on each forward pass a batch of data is randomly sampled from total dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Instructions\n",
    "Open nn.py. See how the network runs with this new architecture.\n",
    "Find the sgd_update method in miniflow.py and implement SGD.\n",
    "Test your network! Does your loss decrease with more epochs?\n",
    "Note! The virtual machines on which we run your code have time limits. If your network takes more than 10 seconds to run, you will get a timeout error. Keep this in mind as you play with the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            self.gradients[self] += n.gradients[self]\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Sum the partial with respect to the input over all the outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # TODO: update all the `trainables` with SGD\n",
    "    # You can access and assign the value of a trainable with `value` attribute.\n",
    "    # Example:\n",
    "    # for t in trainables:\n",
    "    #   t.value = your implementation here\n",
    "    \n",
    "    # Performs SGD\n",
    "    #\n",
    "    # Loop over the trainables\n",
    "    for t in trainables:\n",
    "        # Change the trainable's value by subtracting the learning rate\n",
    "        # multiplied by the partial of the cost with respect to this\n",
    "        # trainable.\n",
    "        partial = t.gradients[t]\n",
    "        t.value -= learning_rate * partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 127.714\n",
      "Epoch: 2, Loss: 31.619\n",
      "Epoch: 3, Loss: 26.373\n",
      "Epoch: 4, Loss: 25.427\n",
      "Epoch: 5, Loss: 25.045\n",
      "Epoch: 6, Loss: 23.497\n",
      "Epoch: 7, Loss: 16.571\n",
      "Epoch: 8, Loss: 15.450\n",
      "Epoch: 9, Loss: 14.162\n",
      "Epoch: 10, Loss: 12.428\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 10\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# SGD Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I'm putting the same quiz below again. If you haven't already, set the number of epochs to something like 1000 and watch as the loss decreases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            self.gradients[self] += n.gradients[self]\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Sum the partial with respect to the input over all the outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # Performs SGD\n",
    "    #\n",
    "    # Loop over the trainables\n",
    "    for t in trainables:\n",
    "        # Change the trainable's value by subtracting the learning rate\n",
    "        # multiplied by the partial of the cost with respect to this\n",
    "        # trainable.\n",
    "        partial = t.gradients[t]\n",
    "        t.value -= learning_rate * partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 112.391\n",
      "Epoch: 2, Loss: 34.063\n",
      "Epoch: 3, Loss: 24.939\n",
      "Epoch: 4, Loss: 22.863\n",
      "Epoch: 5, Loss: 25.494\n",
      "Epoch: 6, Loss: 19.099\n",
      "Epoch: 7, Loss: 21.473\n",
      "Epoch: 8, Loss: 17.199\n",
      "Epoch: 9, Loss: 19.396\n",
      "Epoch: 10, Loss: 16.970\n",
      "Epoch: 11, Loss: 13.185\n",
      "Epoch: 12, Loss: 13.091\n",
      "Epoch: 13, Loss: 10.661\n",
      "Epoch: 14, Loss: 13.061\n",
      "Epoch: 15, Loss: 13.018\n",
      "Epoch: 16, Loss: 11.157\n",
      "Epoch: 17, Loss: 11.848\n",
      "Epoch: 18, Loss: 13.519\n",
      "Epoch: 19, Loss: 9.649\n",
      "Epoch: 20, Loss: 10.796\n",
      "Epoch: 21, Loss: 10.906\n",
      "Epoch: 22, Loss: 9.147\n",
      "Epoch: 23, Loss: 13.383\n",
      "Epoch: 24, Loss: 11.718\n",
      "Epoch: 25, Loss: 12.005\n",
      "Epoch: 26, Loss: 10.879\n",
      "Epoch: 27, Loss: 9.255\n",
      "Epoch: 28, Loss: 10.143\n",
      "Epoch: 29, Loss: 9.779\n",
      "Epoch: 30, Loss: 10.413\n",
      "Epoch: 31, Loss: 9.501\n",
      "Epoch: 32, Loss: 9.086\n",
      "Epoch: 33, Loss: 9.170\n",
      "Epoch: 34, Loss: 9.725\n",
      "Epoch: 35, Loss: 7.327\n",
      "Epoch: 36, Loss: 8.736\n",
      "Epoch: 37, Loss: 10.040\n",
      "Epoch: 38, Loss: 7.102\n",
      "Epoch: 39, Loss: 9.055\n",
      "Epoch: 40, Loss: 9.277\n",
      "Epoch: 41, Loss: 7.547\n",
      "Epoch: 42, Loss: 9.420\n",
      "Epoch: 43, Loss: 9.040\n",
      "Epoch: 44, Loss: 8.678\n",
      "Epoch: 45, Loss: 8.435\n",
      "Epoch: 46, Loss: 8.732\n",
      "Epoch: 47, Loss: 8.971\n",
      "Epoch: 48, Loss: 8.360\n",
      "Epoch: 49, Loss: 9.305\n",
      "Epoch: 50, Loss: 9.687\n",
      "Epoch: 51, Loss: 8.715\n",
      "Epoch: 52, Loss: 7.957\n",
      "Epoch: 53, Loss: 7.299\n",
      "Epoch: 54, Loss: 7.293\n",
      "Epoch: 55, Loss: 7.567\n",
      "Epoch: 56, Loss: 7.940\n",
      "Epoch: 57, Loss: 7.743\n",
      "Epoch: 58, Loss: 8.591\n",
      "Epoch: 59, Loss: 6.640\n",
      "Epoch: 60, Loss: 8.143\n",
      "Epoch: 61, Loss: 8.485\n",
      "Epoch: 62, Loss: 7.124\n",
      "Epoch: 63, Loss: 8.351\n",
      "Epoch: 64, Loss: 6.579\n",
      "Epoch: 65, Loss: 6.173\n",
      "Epoch: 66, Loss: 6.301\n",
      "Epoch: 67, Loss: 6.182\n",
      "Epoch: 68, Loss: 7.696\n",
      "Epoch: 69, Loss: 7.129\n",
      "Epoch: 70, Loss: 8.706\n",
      "Epoch: 71, Loss: 5.584\n",
      "Epoch: 72, Loss: 7.148\n",
      "Epoch: 73, Loss: 7.697\n",
      "Epoch: 74, Loss: 6.589\n",
      "Epoch: 75, Loss: 8.720\n",
      "Epoch: 76, Loss: 7.819\n",
      "Epoch: 77, Loss: 6.724\n",
      "Epoch: 78, Loss: 7.045\n",
      "Epoch: 79, Loss: 6.997\n",
      "Epoch: 80, Loss: 6.145\n",
      "Epoch: 81, Loss: 6.294\n",
      "Epoch: 82, Loss: 6.054\n",
      "Epoch: 83, Loss: 7.629\n",
      "Epoch: 84, Loss: 6.455\n",
      "Epoch: 85, Loss: 6.317\n",
      "Epoch: 86, Loss: 6.828\n",
      "Epoch: 87, Loss: 7.147\n",
      "Epoch: 88, Loss: 6.450\n",
      "Epoch: 89, Loss: 6.137\n",
      "Epoch: 90, Loss: 6.013\n",
      "Epoch: 91, Loss: 7.090\n",
      "Epoch: 92, Loss: 7.023\n",
      "Epoch: 93, Loss: 5.862\n",
      "Epoch: 94, Loss: 7.623\n",
      "Epoch: 95, Loss: 7.272\n",
      "Epoch: 96, Loss: 6.159\n",
      "Epoch: 97, Loss: 6.079\n",
      "Epoch: 98, Loss: 7.219\n",
      "Epoch: 99, Loss: 7.045\n",
      "Epoch: 100, Loss: 5.954\n",
      "Epoch: 101, Loss: 5.966\n",
      "Epoch: 102, Loss: 7.115\n",
      "Epoch: 103, Loss: 6.469\n",
      "Epoch: 104, Loss: 6.035\n",
      "Epoch: 105, Loss: 5.855\n",
      "Epoch: 106, Loss: 5.679\n",
      "Epoch: 107, Loss: 5.435\n",
      "Epoch: 108, Loss: 6.999\n",
      "Epoch: 109, Loss: 5.882\n",
      "Epoch: 110, Loss: 7.030\n",
      "Epoch: 111, Loss: 6.420\n",
      "Epoch: 112, Loss: 6.051\n",
      "Epoch: 113, Loss: 7.100\n",
      "Epoch: 114, Loss: 6.452\n",
      "Epoch: 115, Loss: 5.829\n",
      "Epoch: 116, Loss: 6.297\n",
      "Epoch: 117, Loss: 5.474\n",
      "Epoch: 118, Loss: 5.891\n",
      "Epoch: 119, Loss: 5.287\n",
      "Epoch: 120, Loss: 7.686\n",
      "Epoch: 121, Loss: 5.213\n",
      "Epoch: 122, Loss: 6.648\n",
      "Epoch: 123, Loss: 6.970\n",
      "Epoch: 124, Loss: 5.524\n",
      "Epoch: 125, Loss: 7.025\n",
      "Epoch: 126, Loss: 5.543\n",
      "Epoch: 127, Loss: 5.020\n",
      "Epoch: 128, Loss: 6.226\n",
      "Epoch: 129, Loss: 5.563\n",
      "Epoch: 130, Loss: 6.878\n",
      "Epoch: 131, Loss: 5.839\n",
      "Epoch: 132, Loss: 5.676\n",
      "Epoch: 133, Loss: 5.176\n",
      "Epoch: 134, Loss: 6.010\n",
      "Epoch: 135, Loss: 5.089\n",
      "Epoch: 136, Loss: 5.650\n",
      "Epoch: 137, Loss: 5.945\n",
      "Epoch: 138, Loss: 6.108\n",
      "Epoch: 139, Loss: 4.976\n",
      "Epoch: 140, Loss: 5.912\n",
      "Epoch: 141, Loss: 5.934\n",
      "Epoch: 142, Loss: 6.425\n",
      "Epoch: 143, Loss: 5.346\n",
      "Epoch: 144, Loss: 4.530\n",
      "Epoch: 145, Loss: 5.563\n",
      "Epoch: 146, Loss: 5.631\n",
      "Epoch: 147, Loss: 5.343\n",
      "Epoch: 148, Loss: 6.241\n",
      "Epoch: 149, Loss: 5.931\n",
      "Epoch: 150, Loss: 5.395\n",
      "Epoch: 151, Loss: 5.236\n",
      "Epoch: 152, Loss: 6.021\n",
      "Epoch: 153, Loss: 5.219\n",
      "Epoch: 154, Loss: 4.705\n",
      "Epoch: 155, Loss: 4.900\n",
      "Epoch: 156, Loss: 5.390\n",
      "Epoch: 157, Loss: 4.600\n",
      "Epoch: 158, Loss: 4.910\n",
      "Epoch: 159, Loss: 5.826\n",
      "Epoch: 160, Loss: 5.437\n",
      "Epoch: 161, Loss: 6.357\n",
      "Epoch: 162, Loss: 4.745\n",
      "Epoch: 163, Loss: 5.915\n",
      "Epoch: 164, Loss: 5.095\n",
      "Epoch: 165, Loss: 4.815\n",
      "Epoch: 166, Loss: 5.264\n",
      "Epoch: 167, Loss: 5.419\n",
      "Epoch: 168, Loss: 5.857\n",
      "Epoch: 169, Loss: 4.732\n",
      "Epoch: 170, Loss: 6.113\n",
      "Epoch: 171, Loss: 5.317\n",
      "Epoch: 172, Loss: 4.943\n",
      "Epoch: 173, Loss: 5.610\n",
      "Epoch: 174, Loss: 5.561\n",
      "Epoch: 175, Loss: 5.036\n",
      "Epoch: 176, Loss: 4.977\n",
      "Epoch: 177, Loss: 5.842\n",
      "Epoch: 178, Loss: 4.990\n",
      "Epoch: 179, Loss: 5.637\n",
      "Epoch: 180, Loss: 5.812\n",
      "Epoch: 181, Loss: 4.233\n",
      "Epoch: 182, Loss: 5.033\n",
      "Epoch: 183, Loss: 5.143\n",
      "Epoch: 184, Loss: 5.599\n",
      "Epoch: 185, Loss: 5.262\n",
      "Epoch: 186, Loss: 4.814\n",
      "Epoch: 187, Loss: 5.139\n",
      "Epoch: 188, Loss: 4.641\n",
      "Epoch: 189, Loss: 5.150\n",
      "Epoch: 190, Loss: 4.793\n",
      "Epoch: 191, Loss: 4.900\n",
      "Epoch: 192, Loss: 4.636\n",
      "Epoch: 193, Loss: 5.381\n",
      "Epoch: 194, Loss: 4.577\n",
      "Epoch: 195, Loss: 5.354\n",
      "Epoch: 196, Loss: 5.311\n",
      "Epoch: 197, Loss: 4.515\n",
      "Epoch: 198, Loss: 5.002\n",
      "Epoch: 199, Loss: 4.986\n",
      "Epoch: 200, Loss: 4.583\n",
      "Epoch: 201, Loss: 4.975\n",
      "Epoch: 202, Loss: 4.670\n",
      "Epoch: 203, Loss: 4.968\n",
      "Epoch: 204, Loss: 5.406\n",
      "Epoch: 205, Loss: 4.872\n",
      "Epoch: 206, Loss: 4.470\n",
      "Epoch: 207, Loss: 4.497\n",
      "Epoch: 208, Loss: 4.828\n",
      "Epoch: 209, Loss: 5.241\n",
      "Epoch: 210, Loss: 5.725\n",
      "Epoch: 211, Loss: 4.745\n",
      "Epoch: 212, Loss: 3.985\n",
      "Epoch: 213, Loss: 5.105\n",
      "Epoch: 214, Loss: 4.543\n",
      "Epoch: 215, Loss: 4.882\n",
      "Epoch: 216, Loss: 5.015\n",
      "Epoch: 217, Loss: 4.533\n",
      "Epoch: 218, Loss: 4.840\n",
      "Epoch: 219, Loss: 4.535\n",
      "Epoch: 220, Loss: 4.374\n",
      "Epoch: 221, Loss: 4.261\n",
      "Epoch: 222, Loss: 4.667\n",
      "Epoch: 223, Loss: 4.045\n",
      "Epoch: 224, Loss: 5.571\n",
      "Epoch: 225, Loss: 4.810\n",
      "Epoch: 226, Loss: 4.989\n",
      "Epoch: 227, Loss: 4.408\n",
      "Epoch: 228, Loss: 4.338\n",
      "Epoch: 229, Loss: 4.293\n",
      "Epoch: 230, Loss: 4.714\n",
      "Epoch: 231, Loss: 4.429\n",
      "Epoch: 232, Loss: 4.661\n",
      "Epoch: 233, Loss: 4.148\n",
      "Epoch: 234, Loss: 4.426\n",
      "Epoch: 235, Loss: 4.788\n",
      "Epoch: 236, Loss: 5.126\n",
      "Epoch: 237, Loss: 4.567\n",
      "Epoch: 238, Loss: 4.559\n",
      "Epoch: 239, Loss: 4.089\n",
      "Epoch: 240, Loss: 4.127\n",
      "Epoch: 241, Loss: 4.555\n",
      "Epoch: 242, Loss: 4.712\n",
      "Epoch: 243, Loss: 4.636\n",
      "Epoch: 244, Loss: 4.773\n",
      "Epoch: 245, Loss: 5.201\n",
      "Epoch: 246, Loss: 4.473\n",
      "Epoch: 247, Loss: 4.266\n",
      "Epoch: 248, Loss: 4.193\n",
      "Epoch: 249, Loss: 4.347\n",
      "Epoch: 250, Loss: 4.787\n",
      "Epoch: 251, Loss: 4.386\n",
      "Epoch: 252, Loss: 4.872\n",
      "Epoch: 253, Loss: 4.088\n",
      "Epoch: 254, Loss: 3.972\n",
      "Epoch: 255, Loss: 5.249\n",
      "Epoch: 256, Loss: 4.307\n",
      "Epoch: 257, Loss: 4.580\n",
      "Epoch: 258, Loss: 3.490\n",
      "Epoch: 259, Loss: 4.948\n",
      "Epoch: 260, Loss: 4.510\n",
      "Epoch: 261, Loss: 4.885\n",
      "Epoch: 262, Loss: 4.355\n",
      "Epoch: 263, Loss: 3.949\n",
      "Epoch: 264, Loss: 4.581\n",
      "Epoch: 265, Loss: 4.308\n",
      "Epoch: 266, Loss: 4.873\n",
      "Epoch: 267, Loss: 4.361\n",
      "Epoch: 268, Loss: 4.460\n",
      "Epoch: 269, Loss: 4.194\n",
      "Epoch: 270, Loss: 4.667\n",
      "Epoch: 271, Loss: 3.937\n",
      "Epoch: 272, Loss: 4.195\n",
      "Epoch: 273, Loss: 3.931\n",
      "Epoch: 274, Loss: 3.991\n",
      "Epoch: 275, Loss: 4.031\n",
      "Epoch: 276, Loss: 4.047\n",
      "Epoch: 277, Loss: 4.331\n",
      "Epoch: 278, Loss: 4.130\n",
      "Epoch: 279, Loss: 4.679\n",
      "Epoch: 280, Loss: 4.668\n",
      "Epoch: 281, Loss: 4.238\n",
      "Epoch: 282, Loss: 3.795\n",
      "Epoch: 283, Loss: 4.651\n",
      "Epoch: 284, Loss: 3.889\n",
      "Epoch: 285, Loss: 4.650\n",
      "Epoch: 286, Loss: 4.996\n",
      "Epoch: 287, Loss: 4.068\n",
      "Epoch: 288, Loss: 4.575\n",
      "Epoch: 289, Loss: 4.138\n",
      "Epoch: 290, Loss: 3.747\n",
      "Epoch: 291, Loss: 4.372\n",
      "Epoch: 292, Loss: 4.419\n",
      "Epoch: 293, Loss: 4.333\n",
      "Epoch: 294, Loss: 4.173\n",
      "Epoch: 295, Loss: 3.986\n",
      "Epoch: 296, Loss: 4.017\n",
      "Epoch: 297, Loss: 4.442\n",
      "Epoch: 298, Loss: 4.303\n",
      "Epoch: 299, Loss: 4.730\n",
      "Epoch: 300, Loss: 3.940\n",
      "Epoch: 301, Loss: 4.727\n",
      "Epoch: 302, Loss: 4.277\n",
      "Epoch: 303, Loss: 4.031\n",
      "Epoch: 304, Loss: 3.443\n",
      "Epoch: 305, Loss: 4.181\n",
      "Epoch: 306, Loss: 4.125\n",
      "Epoch: 307, Loss: 4.368\n",
      "Epoch: 308, Loss: 4.695\n",
      "Epoch: 309, Loss: 4.153\n",
      "Epoch: 310, Loss: 4.745\n",
      "Epoch: 311, Loss: 4.058\n",
      "Epoch: 312, Loss: 4.526\n",
      "Epoch: 313, Loss: 4.258\n",
      "Epoch: 314, Loss: 4.236\n",
      "Epoch: 315, Loss: 4.298\n",
      "Epoch: 316, Loss: 4.743\n",
      "Epoch: 317, Loss: 3.998\n",
      "Epoch: 318, Loss: 4.245\n",
      "Epoch: 319, Loss: 4.544\n",
      "Epoch: 320, Loss: 4.904\n",
      "Epoch: 321, Loss: 3.647\n",
      "Epoch: 322, Loss: 4.516\n",
      "Epoch: 323, Loss: 3.318\n",
      "Epoch: 324, Loss: 4.691\n",
      "Epoch: 325, Loss: 4.735\n",
      "Epoch: 326, Loss: 3.348\n",
      "Epoch: 327, Loss: 4.350\n",
      "Epoch: 328, Loss: 4.505\n",
      "Epoch: 329, Loss: 3.962\n",
      "Epoch: 330, Loss: 3.433\n",
      "Epoch: 331, Loss: 3.881\n",
      "Epoch: 332, Loss: 3.941\n",
      "Epoch: 333, Loss: 4.186\n",
      "Epoch: 334, Loss: 4.669\n",
      "Epoch: 335, Loss: 3.476\n",
      "Epoch: 336, Loss: 4.143\n",
      "Epoch: 337, Loss: 4.248\n",
      "Epoch: 338, Loss: 4.563\n",
      "Epoch: 339, Loss: 4.315\n",
      "Epoch: 340, Loss: 4.268\n",
      "Epoch: 341, Loss: 4.124\n",
      "Epoch: 342, Loss: 4.178\n",
      "Epoch: 343, Loss: 4.198\n",
      "Epoch: 344, Loss: 4.839\n",
      "Epoch: 345, Loss: 4.783\n",
      "Epoch: 346, Loss: 3.918\n",
      "Epoch: 347, Loss: 4.370\n",
      "Epoch: 348, Loss: 4.279\n",
      "Epoch: 349, Loss: 4.732\n",
      "Epoch: 350, Loss: 3.593\n",
      "Epoch: 351, Loss: 3.941\n",
      "Epoch: 352, Loss: 4.073\n",
      "Epoch: 353, Loss: 3.947\n",
      "Epoch: 354, Loss: 4.614\n",
      "Epoch: 355, Loss: 4.822\n",
      "Epoch: 356, Loss: 4.021\n",
      "Epoch: 357, Loss: 4.837\n",
      "Epoch: 358, Loss: 4.033\n",
      "Epoch: 359, Loss: 4.150\n",
      "Epoch: 360, Loss: 3.798\n",
      "Epoch: 361, Loss: 4.148\n",
      "Epoch: 362, Loss: 4.062\n",
      "Epoch: 363, Loss: 4.460\n",
      "Epoch: 364, Loss: 4.305\n",
      "Epoch: 365, Loss: 4.295\n",
      "Epoch: 366, Loss: 3.844\n",
      "Epoch: 367, Loss: 4.358\n",
      "Epoch: 368, Loss: 4.189\n",
      "Epoch: 369, Loss: 4.099\n",
      "Epoch: 370, Loss: 3.778\n",
      "Epoch: 371, Loss: 3.628\n",
      "Epoch: 372, Loss: 4.372\n",
      "Epoch: 373, Loss: 3.990\n",
      "Epoch: 374, Loss: 4.121\n",
      "Epoch: 375, Loss: 4.434\n",
      "Epoch: 376, Loss: 3.995\n",
      "Epoch: 377, Loss: 4.121\n",
      "Epoch: 378, Loss: 4.007\n",
      "Epoch: 379, Loss: 3.746\n",
      "Epoch: 380, Loss: 4.033\n",
      "Epoch: 381, Loss: 3.447\n",
      "Epoch: 382, Loss: 3.903\n",
      "Epoch: 383, Loss: 3.923\n",
      "Epoch: 384, Loss: 3.848\n",
      "Epoch: 385, Loss: 4.297\n",
      "Epoch: 386, Loss: 4.964\n",
      "Epoch: 387, Loss: 4.279\n",
      "Epoch: 388, Loss: 4.407\n",
      "Epoch: 389, Loss: 3.482\n",
      "Epoch: 390, Loss: 4.040\n",
      "Epoch: 391, Loss: 4.035\n",
      "Epoch: 392, Loss: 3.737\n",
      "Epoch: 393, Loss: 4.147\n",
      "Epoch: 394, Loss: 3.786\n",
      "Epoch: 395, Loss: 4.147\n",
      "Epoch: 396, Loss: 3.925\n",
      "Epoch: 397, Loss: 3.927\n",
      "Epoch: 398, Loss: 3.879\n",
      "Epoch: 399, Loss: 3.788\n",
      "Epoch: 400, Loss: 4.033\n",
      "Epoch: 401, Loss: 3.875\n",
      "Epoch: 402, Loss: 4.212\n",
      "Epoch: 403, Loss: 3.405\n",
      "Epoch: 404, Loss: 3.785\n",
      "Epoch: 405, Loss: 4.198\n",
      "Epoch: 406, Loss: 3.705\n",
      "Epoch: 407, Loss: 3.742\n",
      "Epoch: 408, Loss: 3.672\n",
      "Epoch: 409, Loss: 4.018\n",
      "Epoch: 410, Loss: 3.488\n",
      "Epoch: 411, Loss: 3.780\n",
      "Epoch: 412, Loss: 3.685\n",
      "Epoch: 413, Loss: 4.004\n",
      "Epoch: 414, Loss: 3.566\n",
      "Epoch: 415, Loss: 4.104\n",
      "Epoch: 416, Loss: 4.081\n",
      "Epoch: 417, Loss: 3.933\n",
      "Epoch: 418, Loss: 3.740\n",
      "Epoch: 419, Loss: 3.421\n",
      "Epoch: 420, Loss: 3.669\n",
      "Epoch: 421, Loss: 4.004\n",
      "Epoch: 422, Loss: 3.777\n",
      "Epoch: 423, Loss: 4.191\n",
      "Epoch: 424, Loss: 3.825\n",
      "Epoch: 425, Loss: 3.926\n",
      "Epoch: 426, Loss: 3.964\n",
      "Epoch: 427, Loss: 3.926\n",
      "Epoch: 428, Loss: 3.802\n",
      "Epoch: 429, Loss: 3.911\n",
      "Epoch: 430, Loss: 3.991\n",
      "Epoch: 431, Loss: 3.864\n",
      "Epoch: 432, Loss: 4.058\n",
      "Epoch: 433, Loss: 3.412\n",
      "Epoch: 434, Loss: 3.844\n",
      "Epoch: 435, Loss: 3.957\n",
      "Epoch: 436, Loss: 4.394\n",
      "Epoch: 437, Loss: 3.795\n",
      "Epoch: 438, Loss: 3.384\n",
      "Epoch: 439, Loss: 3.957\n",
      "Epoch: 440, Loss: 4.025\n",
      "Epoch: 441, Loss: 3.820\n",
      "Epoch: 442, Loss: 3.680\n",
      "Epoch: 443, Loss: 3.647\n",
      "Epoch: 444, Loss: 3.928\n",
      "Epoch: 445, Loss: 3.976\n",
      "Epoch: 446, Loss: 3.543\n",
      "Epoch: 447, Loss: 3.346\n",
      "Epoch: 448, Loss: 3.703\n",
      "Epoch: 449, Loss: 3.810\n",
      "Epoch: 450, Loss: 4.040\n",
      "Epoch: 451, Loss: 3.877\n",
      "Epoch: 452, Loss: 3.950\n",
      "Epoch: 453, Loss: 3.415\n",
      "Epoch: 454, Loss: 4.037\n",
      "Epoch: 455, Loss: 3.271\n",
      "Epoch: 456, Loss: 3.548\n",
      "Epoch: 457, Loss: 3.448\n",
      "Epoch: 458, Loss: 3.734\n",
      "Epoch: 459, Loss: 3.801\n",
      "Epoch: 460, Loss: 3.210\n",
      "Epoch: 461, Loss: 3.668\n",
      "Epoch: 462, Loss: 3.569\n",
      "Epoch: 463, Loss: 3.826\n",
      "Epoch: 464, Loss: 3.815\n",
      "Epoch: 465, Loss: 4.065\n",
      "Epoch: 466, Loss: 2.896\n",
      "Epoch: 467, Loss: 3.616\n",
      "Epoch: 468, Loss: 3.821\n",
      "Epoch: 469, Loss: 4.001\n",
      "Epoch: 470, Loss: 3.405\n",
      "Epoch: 471, Loss: 3.678\n",
      "Epoch: 472, Loss: 3.752\n",
      "Epoch: 473, Loss: 3.305\n",
      "Epoch: 474, Loss: 4.199\n",
      "Epoch: 475, Loss: 3.378\n",
      "Epoch: 476, Loss: 3.680\n",
      "Epoch: 477, Loss: 3.347\n",
      "Epoch: 478, Loss: 3.712\n",
      "Epoch: 479, Loss: 3.822\n",
      "Epoch: 480, Loss: 3.830\n",
      "Epoch: 481, Loss: 4.753\n",
      "Epoch: 482, Loss: 3.818\n",
      "Epoch: 483, Loss: 3.500\n",
      "Epoch: 484, Loss: 4.190\n",
      "Epoch: 485, Loss: 4.280\n",
      "Epoch: 486, Loss: 3.819\n",
      "Epoch: 487, Loss: 3.745\n",
      "Epoch: 488, Loss: 4.361\n",
      "Epoch: 489, Loss: 3.654\n",
      "Epoch: 490, Loss: 3.552\n",
      "Epoch: 491, Loss: 3.801\n",
      "Epoch: 492, Loss: 3.639\n",
      "Epoch: 493, Loss: 3.559\n",
      "Epoch: 494, Loss: 3.943\n",
      "Epoch: 495, Loss: 3.939\n",
      "Epoch: 496, Loss: 3.661\n",
      "Epoch: 497, Loss: 4.042\n",
      "Epoch: 498, Loss: 3.859\n",
      "Epoch: 499, Loss: 3.225\n",
      "Epoch: 500, Loss: 3.909\n",
      "Epoch: 501, Loss: 3.525\n",
      "Epoch: 502, Loss: 3.488\n",
      "Epoch: 503, Loss: 3.473\n",
      "Epoch: 504, Loss: 3.339\n",
      "Epoch: 505, Loss: 3.724\n",
      "Epoch: 506, Loss: 4.004\n",
      "Epoch: 507, Loss: 3.659\n",
      "Epoch: 508, Loss: 3.198\n",
      "Epoch: 509, Loss: 3.846\n",
      "Epoch: 510, Loss: 4.017\n",
      "Epoch: 511, Loss: 3.650\n",
      "Epoch: 512, Loss: 3.911\n",
      "Epoch: 513, Loss: 3.588\n",
      "Epoch: 514, Loss: 3.171\n",
      "Epoch: 515, Loss: 4.012\n",
      "Epoch: 516, Loss: 3.752\n",
      "Epoch: 517, Loss: 3.590\n",
      "Epoch: 518, Loss: 3.645\n",
      "Epoch: 519, Loss: 3.450\n",
      "Epoch: 520, Loss: 3.439\n",
      "Epoch: 521, Loss: 3.875\n",
      "Epoch: 522, Loss: 3.737\n",
      "Epoch: 523, Loss: 3.206\n",
      "Epoch: 524, Loss: 3.713\n",
      "Epoch: 525, Loss: 4.162\n",
      "Epoch: 526, Loss: 3.955\n",
      "Epoch: 527, Loss: 3.715\n",
      "Epoch: 528, Loss: 3.087\n",
      "Epoch: 529, Loss: 3.654\n",
      "Epoch: 530, Loss: 3.541\n",
      "Epoch: 531, Loss: 3.420\n",
      "Epoch: 532, Loss: 3.799\n",
      "Epoch: 533, Loss: 3.592\n",
      "Epoch: 534, Loss: 3.905\n",
      "Epoch: 535, Loss: 3.123\n",
      "Epoch: 536, Loss: 3.563\n",
      "Epoch: 537, Loss: 3.449\n",
      "Epoch: 538, Loss: 3.688\n",
      "Epoch: 539, Loss: 3.829\n",
      "Epoch: 540, Loss: 3.361\n",
      "Epoch: 541, Loss: 3.732\n",
      "Epoch: 542, Loss: 4.235\n",
      "Epoch: 543, Loss: 3.967\n",
      "Epoch: 544, Loss: 3.152\n",
      "Epoch: 545, Loss: 3.218\n",
      "Epoch: 546, Loss: 3.415\n",
      "Epoch: 547, Loss: 3.130\n",
      "Epoch: 548, Loss: 3.537\n",
      "Epoch: 549, Loss: 3.619\n",
      "Epoch: 550, Loss: 3.709\n",
      "Epoch: 551, Loss: 3.442\n",
      "Epoch: 552, Loss: 3.672\n",
      "Epoch: 553, Loss: 3.602\n",
      "Epoch: 554, Loss: 3.555\n",
      "Epoch: 555, Loss: 3.803\n",
      "Epoch: 556, Loss: 4.212\n",
      "Epoch: 557, Loss: 3.574\n",
      "Epoch: 558, Loss: 3.927\n",
      "Epoch: 559, Loss: 3.881\n",
      "Epoch: 560, Loss: 4.107\n",
      "Epoch: 561, Loss: 3.596\n",
      "Epoch: 562, Loss: 3.242\n",
      "Epoch: 563, Loss: 3.529\n",
      "Epoch: 564, Loss: 3.244\n",
      "Epoch: 565, Loss: 3.448\n",
      "Epoch: 566, Loss: 4.124\n",
      "Epoch: 567, Loss: 3.423\n",
      "Epoch: 568, Loss: 3.459\n",
      "Epoch: 569, Loss: 3.720\n",
      "Epoch: 570, Loss: 3.409\n",
      "Epoch: 571, Loss: 3.749\n",
      "Epoch: 572, Loss: 3.401\n",
      "Epoch: 573, Loss: 3.192\n",
      "Epoch: 574, Loss: 3.501\n",
      "Epoch: 575, Loss: 3.488\n",
      "Epoch: 576, Loss: 3.838\n",
      "Epoch: 577, Loss: 3.215\n",
      "Epoch: 578, Loss: 3.548\n",
      "Epoch: 579, Loss: 3.577\n",
      "Epoch: 580, Loss: 3.205\n",
      "Epoch: 581, Loss: 3.493\n",
      "Epoch: 582, Loss: 3.538\n",
      "Epoch: 583, Loss: 3.552\n",
      "Epoch: 584, Loss: 3.609\n",
      "Epoch: 585, Loss: 3.358\n",
      "Epoch: 586, Loss: 3.159\n",
      "Epoch: 587, Loss: 3.341\n",
      "Epoch: 588, Loss: 3.210\n",
      "Epoch: 589, Loss: 2.982\n",
      "Epoch: 590, Loss: 3.362\n",
      "Epoch: 591, Loss: 3.519\n",
      "Epoch: 592, Loss: 4.181\n",
      "Epoch: 593, Loss: 3.569\n",
      "Epoch: 594, Loss: 3.480\n",
      "Epoch: 595, Loss: 3.815\n",
      "Epoch: 596, Loss: 3.571\n",
      "Epoch: 597, Loss: 3.187\n",
      "Epoch: 598, Loss: 3.273\n",
      "Epoch: 599, Loss: 3.329\n",
      "Epoch: 600, Loss: 3.350\n",
      "Epoch: 601, Loss: 3.623\n",
      "Epoch: 602, Loss: 3.682\n",
      "Epoch: 603, Loss: 3.672\n",
      "Epoch: 604, Loss: 3.290\n",
      "Epoch: 605, Loss: 3.341\n",
      "Epoch: 606, Loss: 3.343\n",
      "Epoch: 607, Loss: 3.776\n",
      "Epoch: 608, Loss: 4.193\n",
      "Epoch: 609, Loss: 3.221\n",
      "Epoch: 610, Loss: 3.422\n",
      "Epoch: 611, Loss: 4.052\n",
      "Epoch: 612, Loss: 3.459\n",
      "Epoch: 613, Loss: 2.882\n",
      "Epoch: 614, Loss: 3.032\n",
      "Epoch: 615, Loss: 2.841\n",
      "Epoch: 616, Loss: 3.297\n",
      "Epoch: 617, Loss: 3.047\n",
      "Epoch: 618, Loss: 3.368\n",
      "Epoch: 619, Loss: 2.712\n",
      "Epoch: 620, Loss: 3.627\n",
      "Epoch: 621, Loss: 3.325\n",
      "Epoch: 622, Loss: 3.357\n",
      "Epoch: 623, Loss: 3.853\n",
      "Epoch: 624, Loss: 3.377\n",
      "Epoch: 625, Loss: 3.424\n",
      "Epoch: 626, Loss: 3.783\n",
      "Epoch: 627, Loss: 3.324\n",
      "Epoch: 628, Loss: 3.683\n",
      "Epoch: 629, Loss: 3.460\n",
      "Epoch: 630, Loss: 3.221\n",
      "Epoch: 631, Loss: 4.140\n",
      "Epoch: 632, Loss: 3.473\n",
      "Epoch: 633, Loss: 3.247\n",
      "Epoch: 634, Loss: 3.237\n",
      "Epoch: 635, Loss: 3.370\n",
      "Epoch: 636, Loss: 3.643\n",
      "Epoch: 637, Loss: 3.512\n",
      "Epoch: 638, Loss: 3.705\n",
      "Epoch: 639, Loss: 3.337\n",
      "Epoch: 640, Loss: 3.531\n",
      "Epoch: 641, Loss: 3.022\n",
      "Epoch: 642, Loss: 3.685\n",
      "Epoch: 643, Loss: 3.748\n",
      "Epoch: 644, Loss: 3.568\n",
      "Epoch: 645, Loss: 3.100\n",
      "Epoch: 646, Loss: 3.230\n",
      "Epoch: 647, Loss: 3.058\n",
      "Epoch: 648, Loss: 3.234\n",
      "Epoch: 649, Loss: 3.273\n",
      "Epoch: 650, Loss: 3.226\n",
      "Epoch: 651, Loss: 3.161\n",
      "Epoch: 652, Loss: 3.264\n",
      "Epoch: 653, Loss: 3.189\n",
      "Epoch: 654, Loss: 3.716\n",
      "Epoch: 655, Loss: 3.494\n",
      "Epoch: 656, Loss: 3.500\n",
      "Epoch: 657, Loss: 3.322\n",
      "Epoch: 658, Loss: 3.122\n",
      "Epoch: 659, Loss: 3.411\n",
      "Epoch: 660, Loss: 3.530\n",
      "Epoch: 661, Loss: 2.983\n",
      "Epoch: 662, Loss: 3.584\n",
      "Epoch: 663, Loss: 3.644\n",
      "Epoch: 664, Loss: 3.719\n",
      "Epoch: 665, Loss: 3.072\n",
      "Epoch: 666, Loss: 3.676\n",
      "Epoch: 667, Loss: 3.501\n",
      "Epoch: 668, Loss: 3.104\n",
      "Epoch: 669, Loss: 3.775\n",
      "Epoch: 670, Loss: 3.395\n",
      "Epoch: 671, Loss: 3.279\n",
      "Epoch: 672, Loss: 3.489\n",
      "Epoch: 673, Loss: 3.045\n",
      "Epoch: 674, Loss: 3.758\n",
      "Epoch: 675, Loss: 3.155\n",
      "Epoch: 676, Loss: 3.003\n",
      "Epoch: 677, Loss: 3.501\n",
      "Epoch: 678, Loss: 2.996\n",
      "Epoch: 679, Loss: 3.032\n",
      "Epoch: 680, Loss: 3.594\n",
      "Epoch: 681, Loss: 3.710\n",
      "Epoch: 682, Loss: 3.737\n",
      "Epoch: 683, Loss: 3.276\n",
      "Epoch: 684, Loss: 3.415\n",
      "Epoch: 685, Loss: 3.569\n",
      "Epoch: 686, Loss: 3.613\n",
      "Epoch: 687, Loss: 3.590\n",
      "Epoch: 688, Loss: 3.058\n",
      "Epoch: 689, Loss: 3.438\n",
      "Epoch: 690, Loss: 2.967\n",
      "Epoch: 691, Loss: 3.532\n",
      "Epoch: 692, Loss: 3.519\n",
      "Epoch: 693, Loss: 3.353\n",
      "Epoch: 694, Loss: 3.182\n",
      "Epoch: 695, Loss: 3.108\n",
      "Epoch: 696, Loss: 3.297\n",
      "Epoch: 697, Loss: 2.892\n",
      "Epoch: 698, Loss: 3.663\n",
      "Epoch: 699, Loss: 3.358\n",
      "Epoch: 700, Loss: 3.427\n",
      "Epoch: 701, Loss: 3.348\n",
      "Epoch: 702, Loss: 3.454\n",
      "Epoch: 703, Loss: 3.734\n",
      "Epoch: 704, Loss: 3.080\n",
      "Epoch: 705, Loss: 3.107\n",
      "Epoch: 706, Loss: 3.593\n",
      "Epoch: 707, Loss: 3.139\n",
      "Epoch: 708, Loss: 3.484\n",
      "Epoch: 709, Loss: 3.621\n",
      "Epoch: 710, Loss: 3.304\n",
      "Epoch: 711, Loss: 3.236\n",
      "Epoch: 712, Loss: 3.314\n",
      "Epoch: 713, Loss: 3.726\n",
      "Epoch: 714, Loss: 3.272\n",
      "Epoch: 715, Loss: 3.501\n",
      "Epoch: 716, Loss: 3.016\n",
      "Epoch: 717, Loss: 3.436\n",
      "Epoch: 718, Loss: 3.393\n",
      "Epoch: 719, Loss: 3.854\n",
      "Epoch: 720, Loss: 3.805\n",
      "Epoch: 721, Loss: 3.147\n",
      "Epoch: 722, Loss: 3.448\n",
      "Epoch: 723, Loss: 3.171\n",
      "Epoch: 724, Loss: 3.625\n",
      "Epoch: 725, Loss: 3.746\n",
      "Epoch: 726, Loss: 3.362\n",
      "Epoch: 727, Loss: 3.514\n",
      "Epoch: 728, Loss: 3.119\n",
      "Epoch: 729, Loss: 2.662\n",
      "Epoch: 730, Loss: 3.215\n",
      "Epoch: 731, Loss: 2.874\n",
      "Epoch: 732, Loss: 3.293\n",
      "Epoch: 733, Loss: 3.126\n",
      "Epoch: 734, Loss: 3.215\n",
      "Epoch: 735, Loss: 3.284\n",
      "Epoch: 736, Loss: 3.246\n",
      "Epoch: 737, Loss: 3.016\n",
      "Epoch: 738, Loss: 3.111\n",
      "Epoch: 739, Loss: 3.257\n",
      "Epoch: 740, Loss: 3.424\n",
      "Epoch: 741, Loss: 3.190\n",
      "Epoch: 742, Loss: 2.823\n",
      "Epoch: 743, Loss: 2.935\n",
      "Epoch: 744, Loss: 3.237\n",
      "Epoch: 745, Loss: 3.199\n",
      "Epoch: 746, Loss: 3.531\n",
      "Epoch: 747, Loss: 3.060\n",
      "Epoch: 748, Loss: 3.133\n",
      "Epoch: 749, Loss: 3.699\n",
      "Epoch: 750, Loss: 3.422\n",
      "Epoch: 751, Loss: 3.486\n",
      "Epoch: 752, Loss: 3.371\n",
      "Epoch: 753, Loss: 3.015\n",
      "Epoch: 754, Loss: 3.090\n",
      "Epoch: 755, Loss: 3.449\n",
      "Epoch: 756, Loss: 3.262\n",
      "Epoch: 757, Loss: 3.335\n",
      "Epoch: 758, Loss: 3.437\n",
      "Epoch: 759, Loss: 3.012\n",
      "Epoch: 760, Loss: 3.040\n",
      "Epoch: 761, Loss: 3.266\n",
      "Epoch: 762, Loss: 3.044\n",
      "Epoch: 763, Loss: 3.035\n",
      "Epoch: 764, Loss: 3.557\n",
      "Epoch: 765, Loss: 3.476\n",
      "Epoch: 766, Loss: 3.188\n",
      "Epoch: 767, Loss: 2.921\n",
      "Epoch: 768, Loss: 3.079\n",
      "Epoch: 769, Loss: 3.209\n",
      "Epoch: 770, Loss: 3.041\n",
      "Epoch: 771, Loss: 3.557\n",
      "Epoch: 772, Loss: 3.218\n",
      "Epoch: 773, Loss: 3.398\n",
      "Epoch: 774, Loss: 3.185\n",
      "Epoch: 775, Loss: 3.068\n",
      "Epoch: 776, Loss: 3.537\n",
      "Epoch: 777, Loss: 2.981\n",
      "Epoch: 778, Loss: 2.929\n",
      "Epoch: 779, Loss: 3.130\n",
      "Epoch: 780, Loss: 3.283\n",
      "Epoch: 781, Loss: 3.236\n",
      "Epoch: 782, Loss: 3.118\n",
      "Epoch: 783, Loss: 3.138\n",
      "Epoch: 784, Loss: 3.218\n",
      "Epoch: 785, Loss: 3.316\n",
      "Epoch: 786, Loss: 3.165\n",
      "Epoch: 787, Loss: 3.240\n",
      "Epoch: 788, Loss: 3.396\n",
      "Epoch: 789, Loss: 3.394\n",
      "Epoch: 790, Loss: 3.102\n",
      "Epoch: 791, Loss: 3.558\n",
      "Epoch: 792, Loss: 3.260\n",
      "Epoch: 793, Loss: 3.374\n",
      "Epoch: 794, Loss: 2.841\n",
      "Epoch: 795, Loss: 3.169\n",
      "Epoch: 796, Loss: 3.405\n",
      "Epoch: 797, Loss: 3.266\n",
      "Epoch: 798, Loss: 3.165\n",
      "Epoch: 799, Loss: 3.340\n",
      "Epoch: 800, Loss: 3.352\n",
      "Epoch: 801, Loss: 2.946\n",
      "Epoch: 802, Loss: 3.384\n",
      "Epoch: 803, Loss: 3.116\n",
      "Epoch: 804, Loss: 3.183\n",
      "Epoch: 805, Loss: 3.022\n",
      "Epoch: 806, Loss: 3.439\n",
      "Epoch: 807, Loss: 3.648\n",
      "Epoch: 808, Loss: 3.102\n",
      "Epoch: 809, Loss: 3.373\n",
      "Epoch: 810, Loss: 3.319\n",
      "Epoch: 811, Loss: 3.424\n",
      "Epoch: 812, Loss: 3.110\n",
      "Epoch: 813, Loss: 3.522\n",
      "Epoch: 814, Loss: 3.294\n",
      "Epoch: 815, Loss: 2.809\n",
      "Epoch: 816, Loss: 3.038\n",
      "Epoch: 817, Loss: 2.562\n",
      "Epoch: 818, Loss: 3.184\n",
      "Epoch: 819, Loss: 2.889\n",
      "Epoch: 820, Loss: 3.509\n",
      "Epoch: 821, Loss: 3.074\n",
      "Epoch: 822, Loss: 3.755\n",
      "Epoch: 823, Loss: 3.076\n",
      "Epoch: 824, Loss: 3.624\n",
      "Epoch: 825, Loss: 2.781\n",
      "Epoch: 826, Loss: 3.013\n",
      "Epoch: 827, Loss: 3.261\n",
      "Epoch: 828, Loss: 3.225\n",
      "Epoch: 829, Loss: 3.901\n",
      "Epoch: 830, Loss: 3.088\n",
      "Epoch: 831, Loss: 3.266\n",
      "Epoch: 832, Loss: 2.864\n",
      "Epoch: 833, Loss: 3.081\n",
      "Epoch: 834, Loss: 3.289\n",
      "Epoch: 835, Loss: 3.162\n",
      "Epoch: 836, Loss: 3.312\n",
      "Epoch: 837, Loss: 2.923\n",
      "Epoch: 838, Loss: 3.335\n",
      "Epoch: 839, Loss: 2.841\n",
      "Epoch: 840, Loss: 2.949\n",
      "Epoch: 841, Loss: 3.502\n",
      "Epoch: 842, Loss: 3.250\n",
      "Epoch: 843, Loss: 3.299\n",
      "Epoch: 844, Loss: 2.995\n",
      "Epoch: 845, Loss: 3.244\n",
      "Epoch: 846, Loss: 3.184\n",
      "Epoch: 847, Loss: 3.154\n",
      "Epoch: 848, Loss: 3.376\n",
      "Epoch: 849, Loss: 3.490\n",
      "Epoch: 850, Loss: 2.905\n",
      "Epoch: 851, Loss: 3.320\n",
      "Epoch: 852, Loss: 3.336\n",
      "Epoch: 853, Loss: 3.020\n",
      "Epoch: 854, Loss: 3.266\n",
      "Epoch: 855, Loss: 3.579\n",
      "Epoch: 856, Loss: 3.215\n",
      "Epoch: 857, Loss: 2.751\n",
      "Epoch: 858, Loss: 3.105\n",
      "Epoch: 859, Loss: 3.133\n",
      "Epoch: 860, Loss: 3.071\n",
      "Epoch: 861, Loss: 3.385\n",
      "Epoch: 862, Loss: 2.929\n",
      "Epoch: 863, Loss: 3.098\n",
      "Epoch: 864, Loss: 3.290\n",
      "Epoch: 865, Loss: 3.101\n",
      "Epoch: 866, Loss: 3.068\n",
      "Epoch: 867, Loss: 3.213\n",
      "Epoch: 868, Loss: 3.001\n",
      "Epoch: 869, Loss: 3.501\n",
      "Epoch: 870, Loss: 3.419\n",
      "Epoch: 871, Loss: 3.079\n",
      "Epoch: 872, Loss: 3.461\n",
      "Epoch: 873, Loss: 3.426\n",
      "Epoch: 874, Loss: 3.004\n",
      "Epoch: 875, Loss: 2.957\n",
      "Epoch: 876, Loss: 2.942\n",
      "Epoch: 877, Loss: 3.164\n",
      "Epoch: 878, Loss: 2.924\n",
      "Epoch: 879, Loss: 3.171\n",
      "Epoch: 880, Loss: 3.114\n",
      "Epoch: 881, Loss: 3.342\n",
      "Epoch: 882, Loss: 3.042\n",
      "Epoch: 883, Loss: 3.398\n",
      "Epoch: 884, Loss: 3.322\n",
      "Epoch: 885, Loss: 3.504\n",
      "Epoch: 886, Loss: 3.066\n",
      "Epoch: 887, Loss: 2.898\n",
      "Epoch: 888, Loss: 3.081\n",
      "Epoch: 889, Loss: 3.255\n",
      "Epoch: 890, Loss: 3.090\n",
      "Epoch: 891, Loss: 2.998\n",
      "Epoch: 892, Loss: 3.157\n",
      "Epoch: 893, Loss: 3.010\n",
      "Epoch: 894, Loss: 3.534\n",
      "Epoch: 895, Loss: 3.274\n",
      "Epoch: 896, Loss: 3.365\n",
      "Epoch: 897, Loss: 3.176\n",
      "Epoch: 898, Loss: 2.993\n",
      "Epoch: 899, Loss: 3.466\n",
      "Epoch: 900, Loss: 3.350\n",
      "Epoch: 901, Loss: 2.915\n",
      "Epoch: 902, Loss: 2.983\n",
      "Epoch: 903, Loss: 3.106\n",
      "Epoch: 904, Loss: 3.497\n",
      "Epoch: 905, Loss: 3.131\n",
      "Epoch: 906, Loss: 3.062\n",
      "Epoch: 907, Loss: 3.639\n",
      "Epoch: 908, Loss: 3.267\n",
      "Epoch: 909, Loss: 2.854\n",
      "Epoch: 910, Loss: 2.859\n",
      "Epoch: 911, Loss: 3.493\n",
      "Epoch: 912, Loss: 2.836\n",
      "Epoch: 913, Loss: 3.252\n",
      "Epoch: 914, Loss: 3.077\n",
      "Epoch: 915, Loss: 3.252\n",
      "Epoch: 916, Loss: 3.324\n",
      "Epoch: 917, Loss: 3.391\n",
      "Epoch: 918, Loss: 2.926\n",
      "Epoch: 919, Loss: 2.762\n",
      "Epoch: 920, Loss: 3.271\n",
      "Epoch: 921, Loss: 3.271\n",
      "Epoch: 922, Loss: 2.968\n",
      "Epoch: 923, Loss: 3.222\n",
      "Epoch: 924, Loss: 3.277\n",
      "Epoch: 925, Loss: 3.256\n",
      "Epoch: 926, Loss: 3.034\n",
      "Epoch: 927, Loss: 3.473\n",
      "Epoch: 928, Loss: 3.244\n",
      "Epoch: 929, Loss: 3.233\n",
      "Epoch: 930, Loss: 2.972\n",
      "Epoch: 931, Loss: 2.534\n",
      "Epoch: 932, Loss: 3.268\n",
      "Epoch: 933, Loss: 3.034\n",
      "Epoch: 934, Loss: 2.984\n",
      "Epoch: 935, Loss: 3.053\n",
      "Epoch: 936, Loss: 3.096\n",
      "Epoch: 937, Loss: 3.236\n",
      "Epoch: 938, Loss: 3.320\n",
      "Epoch: 939, Loss: 3.076\n",
      "Epoch: 940, Loss: 3.218\n",
      "Epoch: 941, Loss: 2.870\n",
      "Epoch: 942, Loss: 3.296\n",
      "Epoch: 943, Loss: 3.673\n",
      "Epoch: 944, Loss: 3.122\n",
      "Epoch: 945, Loss: 3.107\n",
      "Epoch: 946, Loss: 2.851\n",
      "Epoch: 947, Loss: 3.125\n",
      "Epoch: 948, Loss: 3.529\n",
      "Epoch: 949, Loss: 3.314\n",
      "Epoch: 950, Loss: 2.856\n",
      "Epoch: 951, Loss: 2.922\n",
      "Epoch: 952, Loss: 3.307\n",
      "Epoch: 953, Loss: 3.015\n",
      "Epoch: 954, Loss: 3.089\n",
      "Epoch: 955, Loss: 3.809\n",
      "Epoch: 956, Loss: 3.019\n",
      "Epoch: 957, Loss: 3.035\n",
      "Epoch: 958, Loss: 3.635\n",
      "Epoch: 959, Loss: 2.931\n",
      "Epoch: 960, Loss: 2.830\n",
      "Epoch: 961, Loss: 3.178\n",
      "Epoch: 962, Loss: 3.178\n",
      "Epoch: 963, Loss: 2.988\n",
      "Epoch: 964, Loss: 3.215\n",
      "Epoch: 965, Loss: 3.274\n",
      "Epoch: 966, Loss: 2.676\n",
      "Epoch: 967, Loss: 3.544\n",
      "Epoch: 968, Loss: 3.404\n",
      "Epoch: 969, Loss: 3.327\n",
      "Epoch: 970, Loss: 2.722\n",
      "Epoch: 971, Loss: 3.577\n",
      "Epoch: 972, Loss: 3.111\n",
      "Epoch: 973, Loss: 3.231\n",
      "Epoch: 974, Loss: 2.867\n",
      "Epoch: 975, Loss: 3.427\n",
      "Epoch: 976, Loss: 3.028\n",
      "Epoch: 977, Loss: 2.906\n",
      "Epoch: 978, Loss: 3.440\n",
      "Epoch: 979, Loss: 3.460\n",
      "Epoch: 980, Loss: 2.792\n",
      "Epoch: 981, Loss: 3.259\n",
      "Epoch: 982, Loss: 2.895\n",
      "Epoch: 983, Loss: 3.407\n",
      "Epoch: 984, Loss: 3.074\n",
      "Epoch: 985, Loss: 2.854\n",
      "Epoch: 986, Loss: 2.699\n",
      "Epoch: 987, Loss: 3.134\n",
      "Epoch: 988, Loss: 3.181\n",
      "Epoch: 989, Loss: 2.803\n",
      "Epoch: 990, Loss: 3.088\n",
      "Epoch: 991, Loss: 3.121\n",
      "Epoch: 992, Loss: 3.387\n",
      "Epoch: 993, Loss: 3.291\n",
      "Epoch: 994, Loss: 2.737\n",
      "Epoch: 995, Loss: 3.102\n",
      "Epoch: 996, Loss: 3.412\n",
      "Epoch: 997, Loss: 2.844\n",
      "Epoch: 998, Loss: 2.902\n",
      "Epoch: 999, Loss: 3.166\n",
      "Epoch: 1000, Loss: 3.252\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Have fun with the number of epochs!\n",
    "\n",
    "Be warned that if you increase them too much,\n",
    "the VM will time out :)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 1000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
